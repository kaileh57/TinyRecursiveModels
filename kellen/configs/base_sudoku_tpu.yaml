# Base configuration for Sudoku experiments on TPU v4-64 (stable-1)

# TPU Infrastructure Configuration
tpu:
  project: "your-gcp-project-id"
  zone: "us-central2-b"
  tpu_name: "stable-1"
  accelerator_type: "v4-64"  # 32 chips, 64 cores, 8 workers
  num_workers: 8

# Data Configuration
data:
  # Local path on stable-1 (persistent node, no GCS needed)
  dataset_path: "/tmp/data/sudoku-extreme-1k-aug-1000"
  local_cache_dir: "/tmp/data/sudoku-extreme-1k-aug-1000"

  # DataLoader settings
  num_workers: 8  # CPU threads per TPU worker
  prefetch_factor: 4  # Prefetch N batches
  pin_memory: false  # Not needed for XLA

# Model Configuration (Paper: Table 1, Sudoku-Extreme)
model:
  # Architecture
  name: "recursive_reasoning.trm@TinyRecursiveReasoningModel_ACTV1"

  # Recursion config (paper: T=3, n=6 for Sudoku)
  H_cycles: 3  # T in paper (high-level cycles)
  L_cycles: 6  # n in paper (latent updates per high-level cycle)

  # Layer config
  H_layers: 0  # Not used in TRM
  L_layers: 2  # Number of transformer/MLP layers

  # Model dimensions
  hidden_size: 512
  num_heads: 8  # Only used if mlp_t=False
  expansion: 4  # MLP expansion ratio

  # Architecture variants
  mlp_t: true  # Use MLP instead of attention (best for 9Ã—9 Sudoku)
  pos_encodings: "none"  # Position encoding: "rope", "learned", or "none"

  # Embeddings
  puzzle_emb_ndim: 512  # Puzzle embedding dimension
  puzzle_emb_len: 16  # Length of puzzle embedding sequence

  # Adaptive Computation Time (ACT) config
  halt_max_steps: 16  # Nsup in paper (deep supervision steps)
  halt_exploration_prob: 0.1  # Exploration for halting
  no_ACT_continue: true  # Simplified ACT (no Q-learning)

  # Training precision
  forward_dtype: "bfloat16"  # Use bfloat16 (TPU v4 optimized)

# Loss Configuration
loss:
  name: "losses@ACTLossHead"
  params:
    loss_type: "stablemax_cross_entropy"

# Training Configuration
training:
  # Batch size
  global_batch_size: 1024  # Total across all workers
  per_worker_batch_size: 128  # global_batch_size / num_workers
  gradient_accumulation_steps: 1  # Accumulate gradients over N steps

  # Training schedule
  epochs: 50000  # Paper uses 50K epochs
  eval_interval: 5000  # Evaluate every N epochs
  checkpoint_interval: 2500  # Save checkpoint every N steps (~30 min)

  # Optimizer
  optimizer: "AdamATan2"
  lr: 4e-4  # Scaled from paper's 1e-4 for batch_size 1024
  lr_min_ratio: 1.0  # No decay (constant LR after warmup)
  lr_warmup_steps: 5000  # Linear warmup steps
  weight_decay: 1.0  # Paper uses 1.0 for Sudoku
  puzzle_emb_weight_decay: 1.0
  beta1: 0.9
  beta2: 0.95

  # Exponential Moving Average (critical!)
  ema: true
  ema_rate: 0.999

  # Random seed
  seed: 42

  # Gradient clipping (optional)
  grad_clip_norm: null

# Checkpoint Configuration (local storage on stable-1)
checkpoint:
  bucket: "stable-1"  # Local storage identifier
  save_dir: "/tmp/checkpoints/sudoku_baseline"
  keep_last_n: 5
  load_from: null

# Logging Configuration (local storage)
logging:
  use_tensorboard: true
  log_interval: 100
  xla_metrics_interval: 1000
  tensorboard_dir: "/tmp/logs/sudoku_baseline"

# Evaluation Configuration
evaluation:
  enabled: true
  eval_set: "test"
  eval_batch_size: 256
  save_outputs: false
