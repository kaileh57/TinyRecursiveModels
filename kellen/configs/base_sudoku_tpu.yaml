# Base configuration for Sudoku experiments on TPU v4-32
# This config replicates the paper's settings scaled for TPU infrastructure

# TPU Infrastructure Configuration
tpu:
  project: "your-gcp-project-id"  # CHANGE THIS
  zone: "us-central2-b"  # TRC-allowed zone, verify your zone
  tpu_name: "kellen-trm-v4-32"
  accelerator_type: "v4-32"  # 32 chips, 8 workers
  num_workers: 8

# Data Configuration
data:
  # GCS path to dataset
  dataset_path: "gs://your-bucket-name/data/sudoku-extreme-1k-aug-1000"  # CHANGE THIS
  local_cache_dir: "/tmp/sudoku_data"  # Local SSD cache on TPU VM

  # DataLoader settings
  num_workers: 8  # CPU threads per TPU worker
  prefetch_factor: 4  # Prefetch N batches
  pin_memory: false  # Not needed for XLA

# Model Configuration (Paper: Table 1, Sudoku-Extreme)
model:
  # Architecture
  name: "TinyRecursiveReasoningModel_ACTV1"

  # Recursion config (paper: T=3, n=6 for Sudoku)
  H_cycles: 3  # T in paper (high-level cycles)
  L_cycles: 6  # n in paper (latent updates per high-level cycle)

  # Layer config
  H_layers: 0  # Not used in TRM (kept for compatibility)
  L_layers: 2  # Number of transformer/MLP layers

  # Model dimensions
  hidden_size: 512
  num_heads: 8  # Only used if mlp_t=False
  expansion: 4  # MLP expansion ratio

  # Architecture variants
  mlp_t: true  # Use MLP instead of attention (best for 9Ã—9 Sudoku)
  pos_encodings: "none"  # Position encoding: "rope", "learned", or "none"

  # Embeddings
  puzzle_emb_ndim: 512  # Puzzle embedding dimension
  puzzle_emb_len: 16  # Length of puzzle embedding sequence

  # Adaptive Computation Time (ACT) config
  halt_max_steps: 16  # Nsup in paper (deep supervision steps)
  halt_exploration_prob: 0.1  # Exploration for halting
  no_ACT_continue: true  # Simplified ACT (no Q-learning)

  # Training precision
  forward_dtype: "bfloat16"  # Use bfloat16 (TPU v4 optimized)

# Training Configuration
training:
  # Batch size
  global_batch_size: 1024  # Total across all workers
  per_worker_batch_size: 128  # global_batch_size / num_workers
  gradient_accumulation_steps: 1  # Accumulate gradients over N steps

  # Training schedule
  epochs: 50000  # Paper uses 50K epochs
  eval_interval: 5000  # Evaluate every N epochs
  checkpoint_interval: 2500  # Save checkpoint every N steps (~30 min)

  # Optimizer (AdamATan2 or AdamW)
  optimizer: "AdamATan2"
  lr: 4e-4  # Scaled from paper's 1e-4 for batch_size 1024
  lr_min_ratio: 1.0  # No decay (constant LR after warmup)
  lr_warmup_steps: 5000  # Linear warmup steps
  weight_decay: 1.0  # Paper uses 1.0 for Sudoku
  puzzle_emb_weight_decay: 1.0  # Weight decay for puzzle embeddings
  beta1: 0.9  # Adam beta1
  beta2: 0.95  # Adam beta2

  # Exponential Moving Average (critical!)
  ema: true
  ema_rate: 0.999  # Paper uses 0.999

  # Random seed
  seed: 42

  # Gradient clipping (optional, add if training unstable)
  grad_clip_norm: null  # Set to 1.0 if needed

# Checkpoint Configuration
checkpoint:
  bucket: "your-bucket-name"  # CHANGE THIS
  save_dir: "checkpoints/sudoku_baseline"  # Path within bucket
  keep_last_n: 5  # Keep last N checkpoints
  load_from: null  # Path to checkpoint to resume from (e.g., "checkpoints/sudoku_baseline/checkpoint_step_10000.pt")

# Logging Configuration
logging:
  use_tensorboard: true
  log_interval: 100  # Log every N steps
  xla_metrics_interval: 1000  # Log XLA metrics every N steps
  tensorboard_dir: "gs://your-bucket-name/logs/sudoku_baseline"  # CHANGE THIS

# Evaluation Configuration
evaluation:
  enabled: true
  eval_set: "test"  # Which split to evaluate on
  eval_batch_size: 256  # Batch size for evaluation
  save_outputs: false  # Save predictions (for analysis)
